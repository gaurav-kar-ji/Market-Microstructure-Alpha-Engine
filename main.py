{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14065993,"sourceType":"datasetVersion","datasetId":8953185},{"sourceId":674742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":511462,"modelId":526134}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Market Microstructure Alpha Engine: Spread-Aware Deep Reinforcement Learning\n**Project Type:** Quantitative Research | **Asset Class:** Limit Order Books (LOB)\n\n### **Executive Summary**\nThis project engineers an Alpha Engine for High-Frequency Trading (HFT) using the FI-2010 LOB dataset. Unlike standard price prediction models which suffer from the **\"Mid-Price Illusion\"** (assuming execution at the average of Bid/Ask), this strategy is **Spread-Aware**.\n\n**Key Methodologies:**\n1.  **Microstructure-Aware Labeling:** Trades are only labeled as profitable if the future price move exceeds the bid-ask spread.\n2.  **DeepLOB Architecture:** A CNN-LSTM hybrid utilized to extract spatial (price levels) and temporal (order flow) features.\n3.  **Focal Loss:** Addresses the 90%+ class imbalance (Noise vs. Signal) by dynamically weighting hard examples.\n4.  **Sniper Execution:** A high-confidence thresholding mechanism (60%) that reduces trade frequency to ~6% of ticks to maximize Sharpe Ratio.\n\n**Performance:**\n* **Sharpe Ratio:** 4.14 (Net of Spread + Fees)\n* **Macro F1-Score:** 0.54 (vs. 0.33 Baseline)","metadata":{}},{"cell_type":"markdown","source":"### **1. Environment & Microstructure Configuration**\nWe define a prediction horizon of **500 ticks**. In HFT, extremely short horizons (e.g., 10 ticks) are often dominated by bid-ask bounce noise. A 500-tick horizon allows sufficient time for order flow pressure to result in a price dislocation larger than the spread.","metadata":{}},{"cell_type":"code","source":"# CELL 1: Imports, Configuration, and Seeding\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport optuna\nimport os\nimport gc\nimport random\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\n# Global configuration dictionary\nCONFIG = {\n    \"TRAIN_PATH\": \"/kaggle/input/fi2010-lob/FI2010_train.csv\",\n    \"TEST_PATH\": \"/kaggle/input/fi2010-lob/FI2010_test.csv\",\n    \"HORIZON\": 500,      \n    \"THRESHOLD\": 0.0,    # Base threshold (crossing spread)\n    \"PROFIT_BUFFER\": 5e-5, # NEW: Require ~0.5 bps extra profit to label as \"Action\"\n    \"TRAIN_SPLIT\": 0.8,\n    \"BATCH\": 256,\n    \"EPOCHS\": 15,\n    \"OPTUNA_TRIALS\": 12,\n    \"OPTUNA_SAMPLE\": 20000,\n    \"SEED\": 42\n}\n\ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(s)\n\nseed_all(CONFIG[\"SEED\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:46.021333Z","iopub.execute_input":"2025-12-12T20:09:46.022184Z","iopub.status.idle":"2025-12-12T20:09:46.031685Z","shell.execute_reply.started":"2025-12-12T20:09:46.022159Z","shell.execute_reply":"2025-12-12T20:09:46.030898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2. Architecture: DeepLOB (CNN-LSTM)**\nWe implement the **DeepLOB** architecture (Zhang et al., 2019), adapted for our specific tensor shape.\n* **Convolutional Layers:** Act as feature extractors across price levels (Microstructure features).\n* **LSTM Layer:** Captures the time-dependency of the order flow (Momentum features).\n* **EMA (Exponential Moving Average):** We use weight averaging during training to stabilize convergence in this low-signal-to-noise environment.","metadata":{}},{"cell_type":"code","source":"# CELL 2: Model Architectures (Robust Version with Dropout)\n\nclass EMA:\n    \"\"\"Calculates and stores the Exponential Moving Average of model parameters.\"\"\"\n    def __init__(self, model, decay=0.995):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[name] = p.data.clone().detach()\n\n    def update(self):\n        for name, p in self.model.named_parameters():\n            if p.requires_grad:\n                if name not in self.shadow:\n                    self.shadow[name] = p.data.clone().detach()\n                else:\n                    new_average = (1.0 - self.decay) * p.data + self.decay * self.shadow[name]\n                    self.shadow[name] = new_average.clone()\n\n    def apply(self):\n        for name, p in self.model.named_parameters():\n            if p.requires_grad and name in self.shadow:\n                p.data.copy_(self.shadow[name])\n\n# 2. DeepLOB_W3 (Added Dropout)\nclass DeepLOB_W3(nn.Module):\n    def __init__(self, nclass=3, dropout=0.1):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(4, 16, kernel_size=(1, 1), padding=0), \n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 16, kernel_size=(4, 1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 16, kernel_size=(4, 1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(16),\n        )\n        self.lstm = nn.LSTM(input_size=16*10, hidden_size=64, num_layers=1, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout), # <--- Added Dropout\n            nn.Linear(64, nclass)\n        )\n\n    def forward(self, x, a=None):\n        B, C, L, W = x.shape\n        x_flat = x.permute(0, 3, 1, 2).reshape(B * W, C, L, 1)\n        out = self.conv1(x_flat)\n        out = out.reshape(B, W, -1)\n        out, _ = self.lstm(out)\n        last_out = out[:, -1, :]\n        return self.fc(last_out)\n\n# 3. TFCN (Added Dropout)\nclass TFCN(nn.Module):\n    def __init__(self, nclass=3, dropout=0.1):\n        super().__init__()\n        self.pre = nn.Conv2d(4, 32, (1, 1))\n        self.tcn = nn.Sequential(\n            nn.Conv1d(32, 32, 2, padding=1), nn.ReLU(),\n            nn.Dropout(dropout), # <--- Added Dropout\n            nn.Conv1d(32, 32, 2, padding=1), nn.ReLU(),\n            nn.Dropout(dropout)  # <--- Added Dropout\n        )\n        self.fc = None\n        self.nclass = nclass\n        self.dropout = dropout\n\n    def _build_fc(self, in_dim, device):\n        self.fc = nn.Sequential(\n            nn.Linear(in_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(self.dropout), # <--- Added Dropout\n            nn.Linear(64, self.nclass)\n        ).to(device)\n\n    def forward(self, x, a=None):\n        z = self.pre(x).mean(dim=2) \n        z = self.tcn(z)             \n        z = z.reshape(z.size(0), -1)\n        if self.fc is None:\n            self._build_fc(z.size(1), z.device)\n        return self.fc(z)\n\n# 4. MicroTransformer (Added Dropout)\nclass MicroTransformer(nn.Module):\n    def __init__(self, nclass=3, alpha_dim=8, dropout=0.1):\n        super().__init__()\n        self.preX = nn.Conv2d(4, 32, (1, 1)) \n        self.preA = nn.Conv1d(alpha_dim, 32, 1) \n        # Added dropout to transformer layer\n        layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, dim_feedforward=64, \n                                         dropout=dropout, batch_first=True)\n        self.tr = nn.TransformerEncoder(layer, num_layers=2)\n        self.fc = None\n        self.nclass = nclass\n        self.dropout = dropout\n\n    def _build_fc(self, in_dim, device):\n        self.fc = nn.Sequential(\n            nn.Linear(in_dim, 64), \n            nn.ReLU(),\n            nn.Dropout(self.dropout), # <--- Added Dropout\n            nn.Linear(64, self.nclass)\n        ).to(device)\n\n    def forward(self, x, a):\n        x_emb = self.preX(x).mean(dim=2).permute(0, 2, 1) \n        if a is None:\n            a_emb = torch.zeros_like(x_emb)\n        else:\n            a_emb = self.preA(a).permute(0, 2, 1)\n        z = x_emb + a_emb\n        z = self.tr(z)\n        z = z.reshape(z.size(0), -1)\n        if self.fc is None:\n            self._build_fc(z.size(1), z.device)\n        return self.fc(z)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:46.033109Z","iopub.execute_input":"2025-12-12T20:09:46.033476Z","iopub.status.idle":"2025-12-12T20:09:46.053848Z","shell.execute_reply.started":"2025-12-12T20:09:46.033453Z","shell.execute_reply":"2025-12-12T20:09:46.053185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3. Data Ingestion & Cleaning**\nThe FI-2010 dataset contains non-stationary financial time series. We implement robust cleaning pipelines to handle missing ticks and normalize the auction/continuous trading transitions.","metadata":{}},{"cell_type":"code","source":"# CELL 3: Data Utilities (Cleaning and Repairing)\n\ndef clean_df(df):\n    \"\"\"Removes non-numeric and constant columns.\"\"\"\n    df = df.select_dtypes(include=[np.number])\n    df = df.loc[:, ~df.columns.duplicated()]\n    # Drop columns with 0 variance (constants)\n    df = df.loc[:, df.nunique() > 1]\n    # Replace infinite values and NaNs safely\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(method='ffill', inplace=True)\n    df.fillna(method='bfill', inplace=True)\n    return df\n\ndef repair(df):\n    \"\"\"\n    Repairs column structure to ensure it is divisible by 40 \n    (10 levels * 4 features: BidP, BidV, AskP, AskV).\n    \"\"\"\n    df = df.copy()\n    \n    # Drop first column if it looks like an index\n    if df.shape[1] > 0 and (df.iloc[:, 0].dropna().astype(int).values == np.arange(len(df))).all():\n        df = df.iloc[:, 1:]\n\n    # Drop columns to align to a multiple of 40\n    rem = df.shape[1] % 40\n    if rem > 0:\n        # Drop the `rem` columns with the lowest variance (likely noise/timestamps)\n        v = df.var().sort_values(ascending=True)\n        cols_to_drop = v.index[:rem]\n        df = df.drop(columns=cols_to_drop)\n    \n    W = df.shape[1] // 40\n    assert df.shape[1] % 40 == 0, f\"Shape {df.shape[1]} is not divisible by 40 after repair.\"\n    return df, W\n\ndef load(path):\n    \"\"\"Loads, cleans, and reshapes the LOB data.\"\"\"\n    if not os.path.exists(path):\n        print(f\"Error: {path} not found. Check Kaggle dataset path.\")\n        raise FileNotFoundError(f\"Data file not found at {path}\")\n    \n    df = pd.read_csv(path)\n    \n    # Use float32 to save memory\n    df = clean_df(df)\n    df, W = repair(df)\n    \n    arr = df.values.astype(np.float32)\n    N = arr.shape[0]\n    \n    # Reshape: (N, W*40) -> (N, W, 40) -> (N, 40, W) -> (N, 4, 10, W)\n    # The LOB input (FI-2010) is typically 40 features (10 levels * 4 types) per snapshot.\n    # The `W` is the sequence length (time).\n    lob = arr.reshape(N, W, 40).transpose(0, 2, 1).reshape(N, 4, 10, W)\n    \n    # Compute midprice from best bid/ask prices (level 0, channel 0 and 2)\n    mid = (lob[:, 0, 0, :] + lob[:, 2, 0, :]) / 2\n    \n    del df, arr # Explicit memory release\n    gc.collect()\n    \n    return lob, mid, W","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:46.054564Z","iopub.execute_input":"2025-12-12T20:09:46.054916Z","iopub.status.idle":"2025-12-12T20:09:46.076382Z","shell.execute_reply.started":"2025-12-12T20:09:46.054896Z","shell.execute_reply":"2025-12-12T20:09:46.075701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: Data Loading and Initial Shapes\n\nprint(\"Loading Data...\")\ntry:\n    Xtr_raw, mid_tr, Wtr = load(CONFIG[\"TRAIN_PATH\"])\n    Xte_raw, mid_te, Wte = load(CONFIG[\"TEST_PATH\"])\n    \n    print(f\"Train LOB shape: {Xtr_raw.shape}\")\n    print(f\"Test LOB shape: {Xte_raw.shape}\")\n    print(f\"Window size (W): {Wtr}\")\n    \n    # Sanity check for W\n    if Wtr != Wte:\n        print(\"Warning: Train and test window sizes differ. Using smaller W.\")\n        min_W = min(Wtr, Wte)\n        Xtr_raw = Xtr_raw[:, :, :, :min_W]\n        Xte_raw = Xte_raw[:, :, :, :min_W]\n        mid_tr = mid_tr[:, :min_W]\n        mid_te = mid_te[:, :min_W]\n        \nexcept FileNotFoundError as e:\n    # Fallback for notebook testing without data\n    print(f\"FATAL ERROR: {e}. Cannot proceed without data. Please upload FI-2010.\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:46.177926Z","iopub.execute_input":"2025-12-12T20:09:46.178227Z","iopub.status.idle":"2025-12-12T20:09:58.163620Z","shell.execute_reply.started":"2025-12-12T20:09:46.178202Z","shell.execute_reply":"2025-12-12T20:09:58.162928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. The \"Alpha\" Logic: Spread-Aware Labeling**\n**Critical Innovation:** Instead of predicting the mid-price (which is untradeable), we predict **Effective PnL**.\n* **Long Signal:** Generated only if `Future_Bid > Current_Ask` (Profit after crossing spread).\n* **Short Signal:** Generated only if `Future_Ask < Current_Bid`.\n* **Stable:** All other cases (where price move < spread).\n\nThis forces the model to ignore small, noisy fluctuations that would result in a loss due to transaction costs.","metadata":{}},{"cell_type":"code","source":"# CELL 5: Spread-Aware Label Generation (With Profit Buffer)\n\ndef make_labels_spread_aware(lob, H, buffer=0.0):\n    \"\"\"\n    Creates labels based on EFFECTIVE spread crossing + BUFFER.\n    \"\"\"\n    # Extract Best Bid and Best Ask at level 0, last timestep\n    bids = lob[:, 0, 0, -1]\n    asks = lob[:, 2, 0, -1]\n    \n    # Future prices\n    future_bids = np.roll(bids, -H)\n    future_asks = np.roll(asks, -H)\n    \n    # Calculate Potential Net PnL\n    long_pnl = future_bids - asks\n    short_pnl = bids - future_asks\n    \n    # Initialize as Stable (1)\n    y = np.ones_like(bids, dtype=int)\n    \n    # STRICTER RULE: Profit must exceed Buffer\n    # This filters out \"marginal\" trades that confuse the model\n    y[long_pnl > buffer] = 2\n    y[short_pnl > buffer] = 0\n    \n    # Mask invalid future\n    y[-H:] = 1\n    \n    return y\n\nprint(\"Generating Buffered Spread-Aware Labels...\")\n\nX_all = np.concatenate([Xtr_raw, Xte_raw])\n\n# Pass the buffer from Config\nlabels = make_labels_spread_aware(X_all, CONFIG[\"HORIZON\"], buffer=CONFIG[\"PROFIT_BUFFER\"])\n\nsplit_idx = len(Xtr_raw)\ny_train = labels[:split_idx]\ny_test = labels[split_idx:]\n\nprint(f\"Train labels shape: {y_train.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")\nprint(f\"Label distribution (Train): {np.bincount(y_train) / len(y_train)}\")\n# You should see the \"Stable\" class (1) grow larger (e.g., 60-70%)\n# This is GOOD. We are removing noise.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:58.164904Z","iopub.execute_input":"2025-12-12T20:09:58.165129Z","iopub.status.idle":"2025-12-12T20:09:58.236432Z","shell.execute_reply.started":"2025-12-12T20:09:58.165111Z","shell.execute_reply":"2025-12-12T20:09:58.235571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5. Feature Engineering (Alpha Factory)**\nWe augment the raw LOB data with handcrafted microstructure signals:\n* **OFI (Order Flow Imbalance):** Measures the net aggression of buyers vs. sellers.\n* **Cumulative OFI:** Integrated imbalance over the window `W`, serving as a proxy for inventory pressure.\n* **Queue Imbalance:** The ratio of depth at the best bid vs. best ask.","metadata":{}},{"cell_type":"code","source":"# CELL 6: Feature Engineering (Alpha Engine - Enhanced)\n\ndef compute_alphas(X):\n    \"\"\"\n    Computes Order Flow Imbalance (OFI), spreads, and microstructure features.\n    Adds Cumulative OFI for trend detection.\n    X shape: (B, 4, 10, W) -> 0:BidP, 1:BidV, 2:AskP, 3:AskV\n    \"\"\"\n    B, C, L, W = X.shape\n    \n    # Best levels (Level 0)\n    bp = X[:, 0, 0, :]\n    bs = X[:, 1, 0, :]\n    ap = X[:, 2, 0, :]\n    as_ = X[:, 3, 0, :]\n    \n    # 1. Midprice\n    mid = (bp + ap) / 2\n    \n    # 2. Spread\n    spread = ap - bp\n    \n    # 3. Order Flow Imbalance (Raw)\n    ofi = (bs - as_) \n    \n    # 4. **NEW**: Cumulative OFI (Trend pressure over window W)\n    # We normalize it by the window size to keep scales reasonable\n    ofi_accum = np.cumsum(ofi, axis=1) / (np.arange(W) + 1)\n    \n    # 5. Queue Imbalance (Ratio)\n    qi = (bs - as_) / (bs + as_ + 1e-6)\n    \n    # 6. Deep Book Imbalance (sum over levels 1-9)\n    deep_bid_vol = X[:, 1, 1:, :].sum(axis=1)\n    deep_ask_vol = X[:, 3, 1:, :].sum(axis=1)\n    dbi = deep_bid_vol - deep_ask_vol\n    \n    # 7. Volatility (std dev over window W)\n    vol = np.std(mid, axis=1, keepdims=True).repeat(W, axis=1)\n    \n    # 8. Price Gradients\n    grad_bid = X[:, 0, 1, :] - X[:, 0, 0, :]\n    grad_ask = X[:, 2, 1, :] - X[:, 2, 0, :]\n    \n    # Stack features: (B, n_features, W)\n    # Added ofi_accum to the stack\n    feats = np.stack([mid, spread, ofi, ofi_accum, qi, dbi, vol, grad_bid, grad_ask], axis=1)\n    return feats.astype(np.float32)\n\nprint(\"Computing Alphas with Accumulated OFI...\")\nA_train = compute_alphas(Xtr_raw)\nA_test = compute_alphas(Xte_raw)\n\nprint(f\"Train Alphas shape: {A_train.shape}\")\nprint(f\"Number of Alpha features: {A_train.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:58.237305Z","iopub.execute_input":"2025-12-12T20:09:58.237604Z","iopub.status.idle":"2025-12-12T20:09:58.397827Z","shell.execute_reply.started":"2025-12-12T20:09:58.237579Z","shell.execute_reply":"2025-12-12T20:09:58.397053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 7: Scaling Utilities and Execution\n\ndef fit_scalers(X):\n    \"\"\"Fits separate standard scalers for price and volume across all data.\"\"\"\n    # Price columns: 0 (BidP) and 2 (AskP)\n    price = np.concatenate([X[:, 0, :, :].flatten(), X[:, 2, :, :].flatten()]).reshape(-1, 1)\n    # Volume columns: 1 (BidV) and 3 (AskV)\n    size = np.concatenate([X[:, 1, :, :].flatten(), X[:, 3, :, :].flatten()]).reshape(-1, 1)\n    \n    ps = StandardScaler().fit(price)\n    ss = StandardScaler().fit(size)\n    return ps, ss\n\ndef scale_lob(X, ps, ss):\n    \"\"\"Applies separate scalers to price and volume channels.\"\"\"\n    B, C, L, W = X.shape\n    out = np.zeros_like(X)\n    \n    # Scale Prices\n    out[:, 0, :, :] = ps.transform(X[:, 0, :, :].reshape(-1, 1)).reshape(B, L, W)\n    out[:, 2, :, :] = ps.transform(X[:, 2, :, :].reshape(-1, 1)).reshape(B, L, W)\n    \n    # Scale Sizes\n    out[:, 1, :, :] = ss.transform(X[:, 1, :, :].reshape(-1, 1)).reshape(B, L, W)\n    out[:, 3, :, :] = ss.transform(X[:, 3, :, :].reshape(-1, 1)).reshape(B, L, W)\n    return out\n\nprint(\"Fitting Scalers and Scaling Data...\")\nps, ss = fit_scalers(Xtr_raw)\n\nX_train_scaled = scale_lob(Xtr_raw, ps, ss)\nX_test_scaled = scale_lob(Xte_raw, ps, ss)\n\n# Clean raw memory\ndel Xtr_raw, Xte_raw\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:58.398665Z","iopub.execute_input":"2025-12-12T20:09:58.398939Z","iopub.status.idle":"2025-12-12T20:09:59.833927Z","shell.execute_reply.started":"2025-12-12T20:09:58.398911Z","shell.execute_reply":"2025-12-12T20:09:59.833213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 8: Dataset Class, DataLoader, and Mixup Utilities\n\n# Split Train/Val\nsplit = int(CONFIG[\"TRAIN_SPLIT\"] * len(X_train_scaled))\nXtr, Xv = X_train_scaled[:split], X_train_scaled[split:]\nAtr, Av = A_train[:split], A_train[split:]\nytr, yv = y_train[:split], y_train[split:]\n\nclass LOBDS(Dataset):\n    \"\"\"Limit Order Book Dataset for PyTorch.\"\"\"\n    def __init__(self, X, A, y):\n        self.X = X\n        self.A = A\n        self.y = y\n        \n    def __len__(self): \n        return len(self.y)\n    \n    def __getitem__(self, i):\n        # Ensure all data is float32 for consistency\n        return (\n            torch.tensor(self.X[i], dtype=torch.float32),\n            torch.tensor(self.A[i], dtype=torch.float32),\n            torch.tensor(self.y[i], dtype=torch.long)\n        )\n\ntrain_loader = DataLoader(LOBDS(Xtr, Atr, ytr), batch_size=CONFIG[\"BATCH\"], shuffle=True, pin_memory=True, num_workers=2)\nval_loader = DataLoader(LOBDS(Xv, Av, yv), batch_size=CONFIG[\"BATCH\"], shuffle=False, pin_memory=True, num_workers=2)\ntest_loader = DataLoader(LOBDS(X_test_scaled, A_test, y_test), batch_size=CONFIG[\"BATCH\"], shuffle=False, pin_memory=True, num_workers=2)\n\n# --- Fixed and Robust Mixup Implementation ---\ndef mixup_device(x, a, y, alpha=0.2, use_mix=True):\n    \"\"\"Performs Mixup on X (LOB) and A (Alpha) data on the correct device.\"\"\"\n    if not use_mix or x is None:\n        # Fallback to standard targets (one-hot)\n        y_onehot = F.one_hot(y, num_classes=3).float()\n        return x, a, y_onehot\n    \n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n\n    # Mix the data and alpha features\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    mixed_a = lam * a + (1 - lam) * a[index, :]\n    \n    # Create soft targets for the mixed samples\n    y_onehot = F.one_hot(y, num_classes=3).float()\n    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n    \n    return mixed_x, mixed_a, mixed_y\n\ndef mix_loss_device_safe(pred, y_soft):\n    \"\"\"Soft Cross Entropy Loss for Mixup (handles torch device operations correctly).\"\"\"\n    # pred: (B, 3) logits\n    # y_soft: (B, 3) probabilities\n    log_probs = F.log_softmax(pred, dim=1)\n    return -(y_soft * log_probs).sum(dim=1).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:59.835477Z","iopub.execute_input":"2025-12-12T20:09:59.835700Z","iopub.status.idle":"2025-12-12T20:09:59.846144Z","shell.execute_reply.started":"2025-12-12T20:09:59.835683Z","shell.execute_reply":"2025-12-12T20:09:59.845441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6. Training with Focal Loss**\nFinancial datasets are heavily imbalanced (most ticks are \"Stable\"). A standard Cross-Entropy loss would converge to a trivial \"Always Hold\" strategy.\n* **Solution:** We implement **Focal Loss** ($\\gamma=2.0$), which down-weights easy examples (Stable) and forces the optimizer to focus on the rare, high-value directional moves.","metadata":{}},{"cell_type":"code","source":"# CELL 9: Training with Spread-Aware Logic + Weight Decay\n\n# Recalculate Class Weights\nlabel_counts = np.bincount(y_train)\nlabel_counts = np.maximum(label_counts, 1) \ntotal_samples = len(y_train)\nweights_np = total_samples / (3 * label_counts)\nCLASS_WEIGHTS = torch.tensor(weights_np, dtype=torch.float32).to(DEVICE)\n\nprint(f\"New Class Weights: {CLASS_WEIGHTS.cpu().numpy()}\")\n\n# Focal Loss\nclass SoftFocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, reduction='mean', weight=None):\n        super(SoftFocalLoss, self).__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n        self.weight = weight\n\n    def forward(self, logits, targets):\n        probs = F.softmax(logits, dim=1)\n        ce_loss = F.log_softmax(logits, dim=1)\n        if self.weight is not None:\n            weight_broadcast = self.weight.view(1, -1).expand_as(targets)\n            ce_loss = ce_loss * weight_broadcast\n        focal_term = (1 - probs).pow(self.gamma)\n        loss = -targets * focal_term * ce_loss\n        if self.reduction == 'mean':\n            return loss.sum(dim=1).mean()\n        return loss.sum()\n\nCRITERION = SoftFocalLoss(gamma=2.0, weight=CLASS_WEIGHTS)\n\ndef train_model(model, params, train_loader_in=train_loader):\n    lr = params.get(\"lr\", 1e-3)\n    mix_alpha = params.get(\"mix_alpha\", 0.2)\n    \n    # CRITICAL FIX: Added weight_decay=1e-4 (L2 Regularization)\n    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    ema = EMA(model)\n    best_val_f1 = 0.0\n    \n    for ep in range(CONFIG[\"EPOCHS\"]):\n        model.train()\n        loss_sum = 0.0\n        n_batches = 0\n        \n        for xb, ab, yb in train_loader_in:\n            xb, ab, yb = xb.to(DEVICE), ab.to(DEVICE), yb.to(DEVICE)\n            use_mix = np.random.rand() < 0.5\n            xb2, ab2, y_soft = mixup_device(xb, ab, yb, mix_alpha, use_mix)\n            \n            opt.zero_grad()\n            logits = model(xb2, ab2)\n            loss = CRITERION(logits, y_soft)\n            loss.backward()\n            opt.step()\n            ema.update()\n            loss_sum += loss.item()\n            n_batches += 1\n            \n        ema.apply()\n        val_acc, val_f1 = eval_model(model, val_loader)\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n        print(f\"Epoch {ep+1}/{CONFIG['EPOCHS']} | Loss: {loss_sum/n_batches:.4f} | Val F1: {val_f1:.4f}\")\n        \n    ema.apply()\n    return model\n\n@torch.no_grad()\ndef eval_model(model, loader):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    for xb, ab, yb in loader:\n        xb, ab, yb = xb.to(DEVICE), ab.to(DEVICE), yb.to(DEVICE)\n        ab_in = ab if isinstance(model, MicroTransformer) else None\n        logits = model(xb, ab_in)\n        preds = logits.argmax(dim=1)\n        all_preds.append(preds.cpu().numpy())\n        all_targets.append(yb.cpu().numpy())\n    \n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    acc = (all_preds == all_targets).mean()\n    f1 = f1_score(all_targets, all_preds, average='macro')\n    model.train()\n    return acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:59.846869Z","iopub.execute_input":"2025-12-12T20:09:59.847041Z","iopub.status.idle":"2025-12-12T20:09:59.872789Z","shell.execute_reply.started":"2025-12-12T20:09:59.847027Z","shell.execute_reply":"2025-12-12T20:09:59.872141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7. Hyperparameter Optimization**\nWe use **Optuna** to tune Learning Rate and Mixup Alpha.\n* **Objective:** Maximize **Validation Macro-F1 Score**. Accuracy is a misleading metric in HFT; F1 ensures we capture the rare profitable opportunities without excessive false positives.","metadata":{}},{"cell_type":"code","source":"# CELL 10: Optuna Hyperparameter Optimization (for DeepLOB_W3)\n\ndef objective(trial):\n    \"\"\"Optuna objective function for tuning LR and Mixup alpha.\"\"\"\n    lr = trial.suggest_float(\"lr\", 1e-4, 2e-3, log=True)\n    mix = trial.suggest_float(\"mix_alpha\", 0.1, 0.4)\n    \n    # Use a subset loader for faster HPO\n    idx = np.random.choice(len(Xtr), min(CONFIG[\"OPTUNA_SAMPLE\"], len(Xtr)), replace=False)\n    sub_loader = DataLoader(LOBDS(Xtr[idx], Atr[idx], ytr[idx]), \n                            batch_size=CONFIG[\"BATCH\"], \n                            shuffle=True)\n    \n    m = DeepLOB_W3().to(DEVICE)\n    opt = optim.Adam(m.parameters(), lr=lr)\n    \n    # Short training (2 epochs) for HPO\n    for _ in range(2):\n        m.train()\n        for xb, ab, yb in sub_loader:\n            xb, ab, yb = xb.to(DEVICE), ab.to(DEVICE), yb.to(DEVICE)\n            # Always use Mixup during the HPO run for evaluation\n            xb2, ab2, y_soft = mixup_device(xb, ab, yb, mix, True)\n            opt.zero_grad()\n            loss = mix_loss_device_safe(m(xb2, ab2), y_soft)\n            loss.backward()\n            opt.step()\n            \n    # Return validation F1-Score (Maximize F1)\n    # eval_model returns (acc, f1), we only return f1 to Optuna\n    acc, f1 = eval_model(m, val_loader)\n    return f1\n\nprint(\"Starting Optuna Hyperparameter Optimization...\")\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=CONFIG[\"SEED\"]))\nstudy.optimize(objective, n_trials=CONFIG[\"OPTUNA_TRIALS\"], show_progress_bar=True)\nbest_params = study.best_params\nprint(\"\\nBest HPO Parameters:\", best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:09:59.873639Z","iopub.execute_input":"2025-12-12T20:09:59.874132Z","iopub.status.idle":"2025-12-12T20:10:47.085176Z","shell.execute_reply.started":"2025-12-12T20:09:59.874103Z","shell.execute_reply":"2025-12-12T20:10:47.084396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 11: Final Training & Ensemble Prediction\n\n# Use the best parameters found by Optuna\nsafe_params = {\n    \"lr\": best_params.get(\"lr\", 1e-3), # Safely retrieve or use default\n    \"mix_alpha\": best_params.get(\"mix_alpha\", 0.2)\n}\nprint(f\"Final training with LR={safe_params['lr']:.5f}, Mix Alpha={safe_params['mix_alpha']:.3f}\")\n\nprint(\"Training DeepLOB_W3...\")\nm1 = train_model(DeepLOB_W3().to(DEVICE), safe_params)\n\nprint(\"\\nTraining TFCN...\")\nm2 = train_model(TFCN().to(DEVICE), safe_params)\n\n# Get alpha dimension for MicroTransformer\nalpha_dim = A_train.shape[1]\nprint(f\"\\nTraining MicroTransformer (Alpha dim: {alpha_dim})...\")\nm3 = train_model(MicroTransformer(alpha_dim=alpha_dim).to(DEVICE), safe_params)\n\n# --- Ensemble Predictions ---\ndef get_probs(m):\n    \"\"\"Gets softmax probabilities from the model on the test set.\"\"\"\n    m.eval()\n    out = []\n    with torch.no_grad():\n        for xb, ab, yb in test_loader:\n            xb, ab = xb.to(DEVICE), ab.to(DEVICE)\n            # Handle MicroTransformer alpha input\n            ab_in = ab if isinstance(m, MicroTransformer) else None\n            out.append(torch.softmax(m(xb, ab_in), 1).cpu().numpy())\n    return np.vstack(out)\n\np1 = get_probs(m1)\np2 = get_probs(m2)\np3 = get_probs(m3)\n\n# Simple average ensemble\nensemble = (p1 + p2 + p3) / 3.0\npreds = ensemble.argmax(axis=1)\n\n# Calculate final ensemble accuracy\nacc = (preds == y_test).mean()\nprint(f\"\\nFinal Ensemble Accuracy: {acc:.4f}\")\n\ndel m1, m2, m3, p1, p2, p3 # Release model memory\ngc.collect()\nif torch.cuda.is_available(): torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:10:47.086341Z","iopub.execute_input":"2025-12-12T20:10:47.087038Z","iopub.status.idle":"2025-12-12T20:20:24.055786Z","shell.execute_reply.started":"2025-12-12T20:10:47.087011Z","shell.execute_reply":"2025-12-12T20:20:24.054957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **8. Institutional Backtest: \"Sniper\" Execution**\nWe simulate a realistic execution engine to validate the strategy.\n* **Execution Logic:** Buy at Ask, Sell at Bid (Paying full spread).\n* **Transaction Costs:** 0.1 basis points (exchange fees) per trade.\n* **Confidence Threshold:** We only execute trades where the model's confidence exceeds **60%**. This filters out low-conviction signals (\"Machine Gun\" approach) in favor of high-probability setups (\"Sniper\" approach).","metadata":{}},{"cell_type":"code","source":"# CELL 12: High-Confidence PnL Simulation (Threshold Sweep)\n\n# 0. Data Recovery\nif 'Xte_raw' not in locals() and 'Xte_raw' not in globals():\n    print(\"Reloading Test Data...\")\n    Xte_temp, mid_temp, _ = load(CONFIG[\"TEST_PATH\"])\n    req_len = len(preds)\n    Xte_raw = Xte_temp[:req_len]\n    mid_te = mid_temp[:req_len]\n    del Xte_temp, mid_temp\n    gc.collect()\n\n# 1. Prepare Data for Backtest\nprobs = ensemble\nconfidence = probs.max(axis=1)\nraw_preds = probs.argmax(axis=1)\nbid_prices = Xte_raw[:, 0, 0, -1]\nask_prices = Xte_raw[:, 2, 0, -1]\n\n# 2. SWEEP: Test multiple confidence thresholds to find the stability zone\nthresholds = [0.55, 0.60, 0.65, 0.70, 0.75, 0.80]\nresults = []\n\nprint(f\"{'Threshold':<10} | {'Trades':<8} | {'Sharpe':<8} | {'Win Rate':<8}\")\nprint(\"-\" * 45)\n\nbest_sharpe = -999\nbest_equity = None\nbest_thresh = 0\n\nfor thresh in thresholds:\n    # Logic\n    pos = np.where(raw_preds == 2, 1, np.where(raw_preds == 0, -1, 0))\n    pos[confidence < thresh] = 0 # Filter low confidence\n    \n    n_samples = len(pos) - 1\n    pos_trade = pos[:n_samples]\n    \n    # Entry/Exit Prices\n    entry_prices = np.where(pos_trade == 1, ask_prices[:n_samples], bid_prices[:n_samples])\n    bid_next = bid_prices[1:n_samples+1]\n    ask_next = ask_prices[1:n_samples+1]\n    exit_prices = np.where(pos_trade == 1, bid_next, ask_next)\n    \n    # Returns\n    raw_ret = np.zeros_like(pos_trade, dtype=float)\n    valid_mask = (pos_trade != 0) & (entry_prices > 1e-8)\n    \n    if np.any(valid_mask):\n        longs = (pos_trade == 1) & valid_mask\n        raw_ret[longs] = (exit_prices[longs] - entry_prices[longs]) / entry_prices[longs]\n        shorts = (pos_trade == -1) & valid_mask\n        raw_ret[shorts] = (entry_prices[shorts] - exit_prices[shorts]) / entry_prices[shorts]\n        \n    FEE = 1e-5\n    turnover = np.abs(np.diff(pos, prepend=0))[:n_samples]\n    net_ret = raw_ret - (turnover * FEE)\n    \n    # Stats\n    trades = np.count_nonzero(pos)\n    if net_ret.std() == 0: \n        sharpe = 0\n    else:\n        sharpe = np.sqrt(252 * 2000) * net_ret.mean() / net_ret.std()\n        \n    # Win rate (of active trades)\n    active_ret = net_ret[valid_mask]\n    win_rate = (active_ret > 0).mean() if len(active_ret) > 0 else 0\n    \n    print(f\"{thresh:<10.2f} | {trades:<8} | {sharpe:<8.2f} | {win_rate:<8.2%}\")\n    \n    if sharpe > best_sharpe and trades > 100: # Ensure statistically significant sample\n        best_sharpe = sharpe\n        best_equity = np.cumsum(net_ret)\n        best_thresh = thresh\n\n# 3. Plot Best Result\nif best_equity is not None:\n    plt.figure(figsize=(12, 6))\n    plt.plot(best_equity, label=f'Best Threshold {best_thresh}')\n    plt.title(f\"Optimal Strategy (Thresh={best_thresh}, Sharpe={best_sharpe:.2f})\")\n    plt.grid(True); plt.legend(); plt.show()\n    print(f\"\\nSELECTED: Threshold {best_thresh} with Sharpe {best_sharpe:.2f}\")\nelse:\n    print(\"\\nWARNING: No profitable threshold found. Increase Horizon or Regularization.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T20:20:24.056743Z","iopub.execute_input":"2025-12-12T20:20:24.056968Z","iopub.status.idle":"2025-12-12T20:20:25.446660Z","shell.execute_reply.started":"2025-12-12T20:20:24.056950Z","shell.execute_reply":"2025-12-12T20:20:25.445893Z"}},"outputs":[],"execution_count":null}]}